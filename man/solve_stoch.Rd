% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/solver_stoch.R
\name{solve_stoch}
\alias{solve_stoch}
\title{Solve MDI with linear equality constraints using a Monte Carlo approach.}
\usage{
solve_stoch(rp, f, m, l_start = rep(1, k + 1), lr = 0.01, S = 100,
  eps = 1e-04, max_iter = 1000)
}
\arguments{
\item{rp}{a function that takes an integer \code{S} as input and returns \code{S} samples from \eqn{p}}

\item{f}{a function returning a vector of size \eqn{k} that evaluates
the LHS of restrictions}

\item{m}{a vector of size \eqn{k} with the RHS of the restrictions}

\item{l_start}{a guess of the optimal \eqn{k + 1} lambdas. Default is \code{rep(0, k + 1)}}

\item{lr}{the learning rate}

\item{S}{the size of the samples to draw from p at each iteration}

\item{eps}{error tolerance (supremum norm)}

\item{max_iter}{maximum number of iterations}
}
\value{
A list
}
\description{
Same as the other but using estimates of the function and jacobian
}
\examples{
#################################################################
# Toy example:
#
# p: multivariate normal (0, \\Sigma)
# Restrictions: force 0.5 mass of the marginals to lie above some
#               randomly chosen thresholds (chi).
# The above implies:
#                   n = k
#                   f(x) = (x > chi)
#                   m = rep(0.5, n)
#################################################################

set.seed(1313) # for reproducibility

n = 9 # number of variables
k = n # number of restrictions
S = 400000 # number of samples

# restrictions
chi = rnorm(n, 1.96, 0.5) # critical values
f = function(x, c = chi){ # vectorized over the size of x (n)
  return(x > c)
}
m = rep(.5, k) # rhs of restrictions

# p-sampler
# p is mvnorm with average cor = rho
rho = 0.6 # average correlation
R = diag(n)
R[upper.tri(R)] = pmin(pmax(rnorm(n*(n-1)/2, rho, 0.1), -1), 1)
R[lower.tri(R)] = t(R)[lower.tri(R)]

rp = function(S){
  return(mvtnorm::rmvnorm(n = S, sigma = R))
}

## SOLVE: stochastic
start.time = proc.time()
fit_MDI = mcMDISolver::solve_stoch(rp = rp, f = f, m = m)
print(proc.time() - start.time)
print(fit_MDI)

## SOLVE: serial implementation
start.time = proc.time()
fit_MDI = mcMDISolver::MDI_solve(mvtnorm::rmvnorm(n = S, sigma = R), f = f, m = m)
print(proc.time() - start.time)
print(fit_MDI$x)

## SOLVE: parallelized implementation

cl = parallel::makeCluster(getOption("cl.cores", 2))

# export additional parameters of f
parallel::clusterExport(cl, list("chi"))

start.time = proc.time()
fit_MDI = mcMDISolver::MDI_solve(mvtnorm::rmvnorm(n = S, sigma = R), f = f, m = m, cl = cl)
print(proc.time() - start.time)
print(fit_MDI$x)
parallel::stopCluster(cl)

## Examine q distribution
# Draw samples using Metropolis algorithm

# log density of q
log_dq = function(x, l = fit_MDI$x){
  return(mvtnorm::dmvnorm(x, sigma = R, log = T) - as.numeric(crossprod(l, c(1, f(x)))))
}

# draw from q
mcmc_S = 2 * S
burn = trunc(0.1 * mcmc_S) # number of samples to burn
sample_q = mcmc::metrop(log_dq, initial = chi, nbatch = mcmc_S + burn)
X_q = sample_q$batch[-(1:burn), ]

# histograms
# note the sudden increase in density around chi
par(mfrow = c(3, 3))
for(i in 1:n) hist(X_q[,i], main = paste("chi =", round(chi[i], 2)), xlab = "")
par(mfrow = c(1, 1))

# check restrictions close to zero
rowMeans(apply(X_q, 1, f)) - m
}
